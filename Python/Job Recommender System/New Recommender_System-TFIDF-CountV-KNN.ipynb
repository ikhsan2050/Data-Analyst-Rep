{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONNECT GOOGLE DRIVE WITH GOOGLE COLAB\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARY\n",
    "\n",
    "# !pip install pdfplumber\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import BigramCollocationFinder\n",
    "from nltk.probability import FreqDist\n",
    "from heapq import nlargest\n",
    "from collections import defaultdict\n",
    "import pandas as pd \n",
    "from nltk.collocations import*\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT CV\n",
    "\n",
    "# Read PDF file and extract text\n",
    "def read_pdf(file_path):\n",
    "    text = extract_text(file_path)\n",
    "    # print(text)\n",
    "    return text\n",
    "resume_path = r\"data\\CV_Mukhamad_Ikhsanudin.pdf\"\n",
    "text = read_pdf(resume_path)\n",
    "print(text)\n",
    "\n",
    "# Additional stopwords\n",
    "f2=open(r\"data\\stopwords.txt\",'r', errors = 'ignore')\n",
    "text2=f2.read()\n",
    "stopwords_additional = word_tokenize(text2.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV PROCESS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp(text):\n",
    "    word_sent = word_tokenize(text.lower().replace(\"\\n\",\"\"))\n",
    "    _stopwords = set(stopwords.words('english', 'indonesian') + list(punctuation)+list(\"●\")+list('–')+list('’')+stopwords_additional)\n",
    "    word_sent=[word for word in word_sent if word not in _stopwords]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    NLP_Processed_CV = [lemmatizer.lemmatize(word) for word in word_tokenize(\" \".join(word_sent))]\n",
    "#     return \" \".join(NLP_Processed_CV)\n",
    "    return NLP_Processed_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_word(x):\n",
    "    finder = BigramCollocationFinder.from_words(x)\n",
    "    keyword = []\n",
    "    keywords_CV = []\n",
    "    for i in sorted(finder.ngram_fd.items()):\n",
    "    # if a double word appears more than once, then print it out.\n",
    "        if i[1] > 1:\n",
    "            print(i)\n",
    "            keyword.append(i[0])\n",
    "        else:\n",
    "            pass\n",
    "    # print(\"************************\")\n",
    "    for j in keyword:\n",
    "    #     print(\" \".join(j))\n",
    "        keywords_CV.append(\" \".join(j))\n",
    "    return keywords_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_Processed_CV = nlp(text)\n",
    "NLP_Processed_CV\n",
    "key_word = get_key_word(NLP_Processed_CV)\n",
    "key_word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT JOB DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scraped data\n",
    "df = pd.read_excel(r\"data\\data website indeed Jakarta (30 May 2023) last.xlsx\", \"Part-Time\")\n",
    "index = []\n",
    "for i in range (0, len(df)):\n",
    "    index.append(i)\n",
    "df.insert(0,'JobID', index)\n",
    "df['All'] = df['Posisi'] + ' ' + df['Perusahaan '] + ' ' + df['Lokasi'] + ' ' + df['Tipe Pekerjaan'] + ' ' + df['Kualifikasi'] + ' ' + df['Deskripsi Pekerjaan ']\n",
    "df.rename(columns = {'Perusahaan ':'Perusahaan', 'Deskripsi Pekerjaan ': 'Deskripsi Pekerjaan'}, inplace = True)\n",
    "df.shape\n",
    "df.info()\n",
    "df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING JOB DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING DATA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8', stop_words=stopwords.words('english', 'indonesian'))\n",
    "tfidf_jobid = tfidf_vectorizer.fit_transform(df['All'].apply(lambda x: np.str_(x)))\n",
    "# print(tfidf_jobid)\n",
    "user_tfidf = tfidf_vectorizer.transform(df['All'].apply(lambda x: np.str_(x)))\n",
    "# print(user_tfidf)\n",
    "\n",
    "cos_similarity_tfidf = map(lambda x: cosine_similarity(user_tfidf,x),tfidf_jobid)\n",
    "outputJob = list(cos_similarity_tfidf)\n",
    "outputJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "\n",
    "# append columns to an empty DataFrame\n",
    "df2['Posisi'] = [\"I\"]\n",
    "df2['Deskripsi Pekerjaan'] = [\"I\"]\n",
    "df2['Perusahaan'] = [\"I\"]\n",
    "\n",
    "# Compare with the key words from CV only\n",
    "df2['All'] = \" \".join(key_word)\n",
    "df2.head()\n",
    "# Compare with the entire CV\n",
    "# df2['All'] = \" \".join(NLP_Processed_CV)\n",
    "# df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECOMMENDATION FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation(top, df_all, scores):\n",
    "    recommendation = pd.DataFrame(columns = ['JobID',  'Posisi', 'Perusahaan', 'Deskripsi Pekerjaan','Score'])\n",
    "    count = 0\n",
    "    for i in top:\n",
    "#         recommendation.at[count, 'ApplicantID'] = u\n",
    "        recommendation.at[count, 'JobID'] = df.index[i]\n",
    "        recommendation.at[count, 'Posisi'] = df['Posisi'][i]\n",
    "        recommendation.at[count, 'Perusahaan'] = df['Perusahaan'][i]\n",
    "#         recommendation.at[count, 'location'] = df['location'][i]\n",
    "        recommendation.at[count, 'Deskripsi Pekerjaan'] = df['Deskripsi Pekerjaan'][i]\n",
    "        recommendation.at[count, 'Score'] =  scores[count]\n",
    "        count += 1\n",
    "    return recommendation\n",
    "\n",
    "def get_top(outputData):\n",
    "    if outputData.__class__ == tuple:\n",
    "        top = outputData[1][0][0:]\n",
    "        list_scores = outputData[0][0][0:]\n",
    "    else:\n",
    "        top = sorted(range(len(outputData)), key=lambda i: outputData[i], reverse=True)[:100]\n",
    "        list_scores = [outputData[i][0][0] for i in top]\n",
    "    return top, list_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF\n",
    "def TFIDF(scraped_data, cv):\n",
    "    tfidf_vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8', stop_words=stopwords.words('english', 'indonesian'))\n",
    "\n",
    "    # TF-IDF Scraped data\n",
    "    tfidf_jobid = tfidf_vectorizer.fit_transform(scraped_data.apply(lambda x: np.str_(x)))\n",
    "\n",
    "    # TF-IDF CV\n",
    "    user_tfidf = tfidf_vectorizer.transform(cv.apply(lambda x: np.str_(x)))\n",
    "\n",
    "    # Using cosine_similarity on (Scraped data) & (CV)\n",
    "    cos_similarity_tfidf = map(lambda x: cosine_similarity(user_tfidf,x),tfidf_jobid)\n",
    "\n",
    "    outputTF = list(cos_similarity_tfidf)\n",
    "    return outputTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTVECTOR\n",
    "def count_vectorize(scraped_data, cv):\n",
    "    count_vectorizer = CountVectorizer(decode_error='replace', encoding='utf-8', stop_words=stopwords.words('english', 'indonesian'))\n",
    "    # CountV the scraped data\n",
    "    count_jobid = count_vectorizer.fit_transform(scraped_data.apply(lambda x: np.str_(x)))\n",
    "\n",
    "    # CountV the cv\n",
    "    user_count = count_vectorizer.transform(cv.apply(lambda x: np.str_(x)))\n",
    "    \n",
    "    # Using cosine_similarity on (Scraped data) & (CV)\n",
    "    cos_similarity_countv = map(lambda x: cosine_similarity(user_count, x),count_jobid)\n",
    "    \n",
    "    output3 = list(cos_similarity_countv)\n",
    "    return output3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "def KNN(scraped_data, cv):\n",
    "    tfidf_vectorizer = TfidfVectorizer(decode_error='replace', encoding='utf-8', stop_words=stopwords.words('english', 'indonesian'))\n",
    "    # n_neighbors = 100\n",
    "    KNN = NearestNeighbors(n_neighbors=10)\n",
    "    KNN.fit(tfidf_vectorizer.fit_transform(scraped_data.apply(lambda x: np.str_(x))))\n",
    "#     NNs = KNN.kneighbors(tfidf_vectorizer.transform(cv), return_distance=True)\n",
    "    NNs = KNN.kneighbors(tfidf_vectorizer.transform(cv.apply(lambda x: np.str_(x))))\n",
    "    # knn = get_recommendation(top, df, index_score)\n",
    "    return NNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV TEST WITH TF-IDF, COUNT VECTORIZER, KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputTF = TFIDF(df['All'], df2['All'])\n",
    "outputTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputCV = count_vectorize(df['All'], df2['All'])\n",
    "outputCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputKNN = KNN(df['All'], df2['All'])\n",
    "# outputKNN\n",
    "outputKNN = KNN(df['All'], df2['All'])\n",
    "outputKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top, list_score = get_top(outputTF)\n",
    "finalTF = get_recommendation(top, df, list_score)\n",
    "finalTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top, list_score = get_top(outputCV)\n",
    "finalCV = get_recommendation(top, df, list_score)\n",
    "finalCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top = outputKNN[1][0][0:]\n",
    "# list_score = outputKNN[0][0][0:]\n",
    "top, list_score = get_top(outputKNN)\n",
    "finalKNN = get_recommendation(top, df, list_score)\n",
    "finalKNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge1 = finalKNN[['JobID','Posisi', 'Score']].merge(finalTF[['JobID','Score']], on= \"JobID\")\n",
    "final = merge1.merge(finalCV[['JobID','Score']], on = \"JobID\")\n",
    "final = final.rename(columns={\"Score_x\": \"KNN\", \"Score_y\": \"TF-IDF\",\"Score\": \"CV\"})\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCALE\n",
    "slr = MinMaxScaler()\n",
    "final[[\"KNN\", \"TF-IDF\", 'CV']] = slr.fit_transform(final[[\"KNN\", \"TF-IDF\", 'CV']])\n",
    "\n",
    "# Multiply by weights\n",
    "final['KNN'] = (1-final['KNN'])/3\n",
    "final['TF-IDF'] = final['TF-IDF']/3\n",
    "final['CV'] = final['CV']/3\n",
    "final['Final'] = final['KNN']+final['TF-IDF']+final['CV']\n",
    "final.sort_values(by=\"Final\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_all = df.merge(final, on=\"JobID\")\n",
    "final_job = final_all.sort_values(by=\"Final\", ascending=False).copy()\n",
    "final_job"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPORT TO PICKLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(finalTF, open('artifacts/TF.pkl', 'wb'))\n",
    "pickle.dump(finalCV, open('artifacts/cv.pkl', 'wb'))\n",
    "pickle.dump(finalKNN, open('artifacts/knn.pkl', 'wb'))\n",
    "pickle.dump(final, open('artifacts/final.pkl', 'wb'))\n",
    "pickle.dump(final_job, open('artifacts/final_job.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
